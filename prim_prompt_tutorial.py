# -*- coding: utf-8 -*-
"""Prim prompt-tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fq0Hq1dFLQDoAIjCXqGwxOecA-gZCFh

Original Tutorial:
https://github.com/Shaveek23/Prompt-based-learning/tree/e84185678f15c47fbb7e7ad916d5e26760ab5f58

OpenPrompt Library:
https://thunlp.github.io/OpenPrompt/notes/examples.html
"""

# install library
!pip install -q torch openprompt

import gdown

file_id = "1V6YzEoM2Dbi2xidBg7b8XNHprkWAf9sN"
output_file = "Twitter_Dataset.csv"

gdown.download(f"https://drive.google.com/uc?id={file_id}", output_file)

import pandas as pd

dataset_path = '/content/Twitter_Dataset.csv'
df = pd.read_csv(dataset_path)

print(df.head())

"""# Step 1: Define a Task

Sentiment Analysis (Binary Classification)
"""

from openprompt.data_utils import InputExample
import pandas as pd

classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive
    "positive",
    "negative"
]

# Function to transform the DataFrame to InputExamples with sentiment labels
def transform_dataframe_to_inputexamples(df):
    examples = []
    for index, row in df.iterrows():
        sentiment_label = classes[row['label']]  # Convert numeric label to sentiment
        example = InputExample(guid=index, text_a=row['tweet'], label=sentiment_label)
        examples.append(example)
    return examples

# Transform the dataframe
dataset = transform_dataframe_to_inputexamples(df)

'''
dataset = [ # For simplicity, there's only two examples
    # text_a , text_b is the input text of the data, some other datasets may have multiple input sentences in one example.
    # guid is the input index
    # label is for its corresponding label for training model
    InputExample(
        guid = 0,
        text_a = "Worth watching!",
    ),
    InputExample(
        guid = 1,
        text_a = "The film was badly made.",
    ),
    InputExample(
        guid = 2,
        text_a = "Simply the best movie in a generation.",
    ),
    InputExample(
        guid = 3,
        text_a = "We were totally confused throughout the movie and thought it was rubbish.",
    ),
]
'''

# Function to transform the DataFrame to InputExamples without changing label types
def transform_dataframe_to_inputexamples(df):
    examples = []
    for index, row in df.iterrows():
        example = InputExample(guid=index, text_a=row['tweet'], label=row['label'])
        examples.append(example)
    return examples

dataset = transform_dataframe_to_inputexamples(df)

dataset = [ # For simplicity, there's only two examples
    # text_a , text_b is the input text of the data, some other datasets may have multiple input sentences in one example.
    # guid is the input index
    # label is for its corresponding label for training model
    InputExample(
        guid = 0,
        text_a = "Worth watching!",
    ),
    InputExample(
        guid = 1,
        text_a = "The film was badly made.",
    ),
    InputExample(
        guid = 2,
        text_a = "Simply the best movie in a generation.",
    ),
    InputExample(
        guid = 3,
        text_a = "We were totally confused throughout the movie and thought it was rubbish.",
    ),
]

dataset

dataset

"""# Step 2: Define a Pre-trained Language Models (PLMs) as backbone

- Masked Language Models (MLM): BERT, RoBERTa, ALBERT
- Autoregressive Language Models (LM): GPT, GPT2
- Sequence-to-Sequence Models (Seq2Seq): T5

Find more model : https://huggingface.co/models

"""

from openprompt.plms import load_plm
# WrapperClass prepares input data into prompt format
plm, tokenizer, model_config, WrapperClass = load_plm("bert", "bert-large-uncased")

"""# Step 3: Define a Template

A Template is a modifier of the original input text, which is also one of the most important modules in prompt-learning. A more advanced tutorial to define a template is in [How to Write a Template](https://thunlp.github.io/OpenPrompt/notes/template.html#tutorial-template) ?
"""

from openprompt.prompts import ManualTemplate
promptTemplate = ManualTemplate(
    text = '{"placeholder":"text_a"} It was {"mask"}.',
    tokenizer = tokenizer,
)

"""# Step 4: Define a Verbalizer

A Verbalizer is another important (but not necessary such as in generation) in prompt-learning,which projects the original labels (we have defined them as classes, remember?) to a set of label words. A more advanced tutorial to define a verbalizer is in [How to Write a Verbalizer](https://thunlp.github.io/OpenPrompt/notes/verbalizer.html#how-to-write-a-verbalizer) ?
"""

from openprompt.prompts import ManualVerbalizer
promptVerbalizer = ManualVerbalizer(
    classes = classes,
    label_words = {
        "positive": ["wow", "fun", "great"],
        "negative": ["terrible"]
    },
    tokenizer = tokenizer,
)

"""# Step 5. Construct a PromptModel
Given the task, now we have a PLM, a Template and a Verbalizer, we combine them into a PromptModel. Note that although the example naively combine the three modules, you can actually define some complicated interactions among them.

- PromptForClassification
- PromptForGeneration
"""

from openprompt import PromptForClassification
promptModel = PromptForClassification(
    template = promptTemplate,
    plm = plm,
    verbalizer = promptVerbalizer,
)

"""# Step 6: Define a DataLoader
A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper.
"""

from openprompt import PromptDataLoader
data_loader = PromptDataLoader(
    dataset = dataset,
    tokenizer = tokenizer,
    template = promptTemplate,
    tokenizer_wrapper_class=WrapperClass,
)

from openprompt import PromptDataLoader
data_loader = PromptDataLoader(
    dataset = dataset,
    tokenizer = tokenizer,
    template = promptTemplate,
    tokenizer_wrapper_class=WrapperClass,
)

"""# Step 7: Train and inference
Done! We can conduct training and inference the same as other processes in Pytorch.
"""

import torch

def calculate_accuracy(promptModel, data_loader, dataset):
    correct_predictions = 0
    total_predictions = 0

    promptModel.eval()
    with torch.no_grad():
        for batch in data_loader:
            logits = promptModel(batch)
            preds = torch.argmax(logits, dim=-1)

            # Assuming 'batch' contains 'guid' which maps directly to the index in dataset
            # and each batch size is 1 for simplicity of this example.
            for idx, pred in enumerate(preds):
                # Extract corresponding InputExample using guid from batch
                example = next((x for x in dataset if x.guid == batch["guid"][idx]), None)
                if example is not None:
                    #print(f"Tweet: {example.text_a}")
                    #print(f"True Sentiment: {classes[example.label]}, Predicted Sentiment: {classes[pred.item()]}")
                    #print()

                    # Count correct predictions
                    if example.label == pred.item():
                        correct_predictions += 1
                    total_predictions += 1

    if total_predictions > 0:
        accuracy = correct_predictions / total_predictions
    else:
        accuracy = 0  # Avoid division by zero

    return accuracy

# Assuming 'promptModel' and 'data_loader' are defined, as well as 'dataset'
accuracy = calculate_accuracy(promptModel, data_loader, dataset)
print("Accuracy:", accuracy)

import torch

# making zero-shot inference using pretrained MLM with prompt
promptModel.eval()
with torch.no_grad():
    for batch in data_loader:
        logits = promptModel(batch)
        preds = torch.argmax(logits, dim = -1)
        print(next(x.text_a for x in dataset if x.guid==batch["guid"]))
        print("Sentiment: ", classes[preds])
        print()